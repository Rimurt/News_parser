import cloudscraper
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import time
import random

ua = UserAgent()

# –°–æ–∑–¥–∞—ë–º cloudscraper (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ö–æ–¥–∏—Ç Cloudflare)
scraper = cloudscraper.create_scraper()

BASE_URL = "https://www.igromania.ru"


def get_headers():
    return {
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
        'Referer': 'https://www.google.com/',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }


def safe_get(url, retries=5, delay=5):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π GET-–∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
    for attempt in range(1, retries + 1):
        try:
            response = scraper.get(url, headers=get_headers(), timeout=20)
            response.raise_for_status()
            return response
        except Exception as e:
            print(f"[{attempt}/{retries}] –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            if attempt < retries:
                sleep_time = delay + random.uniform(1, 4)
                print(f"–ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {sleep_time:.1f} —Å–µ–∫...")
                time.sleep(sleep_time)
            else:
                print("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ —á–∏—Å–ª–æ –ø–æ–ø—ã—Ç–æ–∫. –ü—Ä–æ–ø—É—Å–∫.")
                return None


def get_links():
    """–ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    response = safe_get(BASE_URL)
    if not response:
        return []

    soup = BeautifulSoup(response.text, "lxml")
    main_div = soup.find("div", class_="app-main")
    if not main_div:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –≥–ª–∞–≤–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π.")
        return []

    container = main_div.find("div", class_="app-container")#type:ignore
    if not container:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏.")
        return []

    # –ò—â–µ–º –≤—Å–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
    news_blocks = container.find_all("a", href=True)#type:ignore
    all_links = []

    for a in news_blocks:
        href = a.get("href")#type:ignore
        if href and href.startswith("/news/"):#type:ignore
            all_links.append(BASE_URL + href) #type:ignore

    all_links = list(set(all_links))
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(all_links)}")
    return all_links


def get_title(news_url):
    """–ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏."""
    response = safe_get(news_url)
    if not response:
        return None

    soup = BeautifulSoup(response.text, "lxml")
    h1 = soup.find("h1")
    if h1:
        return h1.text.strip()
    return None


# –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
if __name__ == "__main__":
    print("üì° –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
    news_links = get_links()

    if not news_links:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫–∏. –í–æ–∑–º–æ–∂–Ω–æ, —Å–∞–π—Ç –∏–∑–º–µ–Ω–∏–ª —Å—Ç—Ä—É–∫—Ç—É—Ä—É.")
        exit()

    titles = []
    for link in news_links[:10]:  # –æ–≥—Ä–∞–Ω–∏—á–∏–º 10 –Ω–æ–≤–æ—Å—Ç—è–º–∏
        print(f"\nüì∞ –ü–∞—Ä—Å–∏–º: {link}")
        time.sleep(random.uniform(4, 8))  # –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        title = get_title(link)
        if title:
            titles.append(title)
            print(f"‚Üí {title}")
        else:
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫")

    print("\n=== –ò—Ç–æ–≥–æ–≤—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ ===")
    for t in titles:
        print("‚Ä¢", t)

import cloudscraper
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from db import session, News
import time
import random
import re


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä User-Agent –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
ua = UserAgent()

# –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä cloudscraper, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –æ–±–µ—Ä—Ç–∫–æ–π –Ω–∞–¥ requests
# –∏ —É–º–µ–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ö–æ–¥–∏—Ç—å –∑–∞—â–∏—Ç—É –æ—Ç –±–æ—Ç–æ–≤ Cloudflare.
scraper = cloudscraper.create_scraper()

BASE_URL = "https://www.igromania.ru"



def get_headers():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç HTTP-–∑–∞–≥–æ–ª–æ–≤–∫–∏, –∏–º–∏—Ç–∏—Ä—É—é—â–∏–µ —Ä–µ–∞–ª—å–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä,
    —á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã —Å–∞–π—Ç–∞."""
    return {
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
        'Referer': 'https://www.google.com/',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }


def safe_get(url, retries=5, delay=5):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π GET-–∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
    for attempt in range(1, retries + 1):
        try:
            response = scraper.get(url, headers=get_headers(), timeout=20)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤–µ—Ä–Ω—É–ª –ª–∏ —Å–µ—Ä–≤–µ—Ä –∫–æ–¥ –æ—à–∏–±–∫–∏ (4xx –∏–ª–∏ 5xx)
            response.raise_for_status()
            return response
        except Exception as e:
            print(f"[{attempt}/{retries}] –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            if attempt < retries:
                sleep_time = delay + random.uniform(1, 4)
                print(f"–ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {sleep_time:.1f} —Å–µ–∫...")
                time.sleep(sleep_time)
            else:
                print("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ —á–∏—Å–ª–æ –ø–æ–ø—ã—Ç–æ–∫. –ü—Ä–æ–ø—É—Å–∫.")
                return None


def get_links():
    """–ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    response = safe_get(BASE_URL)
    if not response:
        return []

    soup = BeautifulSoup(response.text, "lxml")
    main_div = soup.find("div", class_="app-main")
    if not main_div:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –≥–ª–∞–≤–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π.")
        return []

    container = main_div.find("div", class_="app-container")#type:ignore
    if not container:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏.")
        return []

    # –ù–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∏–¥–æ–≤ –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏. –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ.
    big_news = container.find_all("div", class_="style_card__mRsjZ knb-card knb-grid-cell cell--row-2 cell--col-2")#type:ignore
    little_news = container.find_all("div", class_="style_card__ZD6TK knb-card knb-grid-cell withShadow cell--row-2 cell--col-1")#type:ignore
    news_without_img = container.find_all("div", class_="style_card__iYFwf knb-card knb-grid-cell withShadow cell--row-2 cell--col-1")#type:ignore
    all_links = []

    for a in big_news:
        link = "https://www.igromania.ru" + a.find("a",class_="knb-card--image style_wrap___iepK style_isAbsolute__P_sj_").get("href")#type:ignore
        all_links.append(link) #type:ignore
    
    for a in little_news:
        link = "https://www.igromania.ru" + a.find("a").get("href")#type:ignore
        all_links.append(link) #type:ignore
    
    for a in news_without_img:
        link = "https://www.igromania.ru" + a.find("a").get("href")#type:ignore
        all_links.append(link) #type:ignore

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏ –æ–±—Ä–∞—Ç–Ω–æ, —á—Ç–æ–±—ã —É–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Å—ã–ª–æ–∫.
    all_links = list(set(all_links))
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(all_links)}")
    # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –≤—ã–∑–æ–≤ –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –ø–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞ –Ω–µ –¥–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    # –ú–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ –ø—Ä–∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–∏ –∏–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å–∞–π—Ç–∞.
    if len(all_links) == 0:
        return get_links()
    else:
        return all_links

def extract_id(url: str) -> str | None:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ–≤–æ–π ID –∏–∑ —Å—Å—ã–ª–æ–∫ –ò–≥—Ä–æ–º–∞–Ω–∏–∏:
    /news/<id>/..., /review/<id>/..., /article/<id>/...
    """
    match = re.search(r"/(?:news|review|article)/(\d+)/", url)
    return match.group(1) if match else None

def get_news_content(news_url):

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –Ω–æ–≤–æ—Å—Ç—å —Å —Ç–∞–∫–∏–º ID –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è.
    if session.get(News,extract_id(news_url)):
        print("–ù–æ–≤–æ—Å—Ç—å —É–∂–µ –µ—Å—Ç—å –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
        return
    
    """–ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏."""
    response = safe_get(news_url)
    if not response:
        return None

    soup = BeautifulSoup(response.text, "lxml")
    news = {
        "id": "",
        "title": "",
        "content": "",
        "image": "",
    }
    
    news["id"] = str(extract_id(news_url))
    h1 = soup.find("h1")
    if h1:
        news["title"] = h1.text
    content_grid = soup.find("div",class_="d-grid template-columns-5 gap-20 w-100")
    content_text = content_grid.find_all("p") #type:ignore
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
    # .get_text(" ", strip=True) –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Ç–µ–≥–æ–≤ <p>, –∑–∞–º–µ–Ω—è—è <br> –∏ –¥—Ä—É–≥–∏–µ —Ç–µ–≥–∏ –Ω–∞ –ø—Ä–æ–±–µ–ª.
    raw_text = "\n\n".join(p.get_text(" ", strip=True) for p in content_text)

    # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –º—É—Å–æ—Ä–Ω—ã—Ö —Å—Ç—Ä–æ–∫, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
    clean_text = re.sub(r"–ò—Å—Ç–æ—á–Ω–∏–∫:.*?(?=\n|$)", "", raw_text)
    # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ–¥–∏–Ω –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã.
    clean_text = re.sub(r"\s+", " ", clean_text).strip()

    news["content"] = clean_text

    image = soup.find("img",class_="MaterialCommonImage_picture__Z_3EU")
    if image:
        news["image"] = image.get("src")#type:ignore
    else:
        news["image"] = ""
    
    data = News(
        id=news["id"],
        title=news["title"],
        content=news["content"],
        image=news["image"],
    )
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –æ–±—ä–µ–∫—Ç News –≤ —Å–µ—Å—Å–∏—é –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –ë–î.
    session.add(data)
    session.commit()
    print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö c id: {news['id']}")


# –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
def parsing():
    print("üì° –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
    news_links = get_links()

    if not news_links:
        return "error"

    for link in news_links:
        print(f"\nüì∞ –ü–∞—Ä—Å–∏–º: {link}")
        time.sleep(random.uniform(4, 8))  # –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        get_news_content(link)
    return "ok"
    

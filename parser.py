import cloudscraper
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import time
import random
import re

ua = UserAgent()

# –°–æ–∑–¥–∞—ë–º cloudscraper (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ö–æ–¥–∏—Ç Cloudflare)
scraper = cloudscraper.create_scraper()

BASE_URL = "https://www.igromania.ru"


def get_headers():
    return {
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
        'Referer': 'https://www.google.com/',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }


def safe_get(url, retries=5, delay=5):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π GET-–∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
    for attempt in range(1, retries + 1):
        try:
            response = scraper.get(url, headers=get_headers(), timeout=20)
            response.raise_for_status()
            return response
        except Exception as e:
            print(f"[{attempt}/{retries}] –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            if attempt < retries:
                sleep_time = delay + random.uniform(1, 4)
                print(f"–ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {sleep_time:.1f} —Å–µ–∫...")
                time.sleep(sleep_time)
            else:
                print("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ —á–∏—Å–ª–æ –ø–æ–ø—ã—Ç–æ–∫. –ü—Ä–æ–ø—É—Å–∫.")
                return None


def get_links():
    """–ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    response = safe_get(BASE_URL)
    if not response:
        return []

    soup = BeautifulSoup(response.text, "lxml")
    main_div = soup.find("div", class_="app-main")
    if not main_div:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –≥–ª–∞–≤–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π.")
        return []

    container = main_div.find("div", class_="app-container")#type:ignore
    if not container:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏.")
        return []

    # –ò—â–µ–º –≤—Å–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
    big_news = container.find_all("div", class_="style_card__mRsjZ knb-card knb-grid-cell cell--row-2 cell--col-2")#type:ignore
    little_news = container.find_all("div", class_="style_card__ZD6TK knb-card knb-grid-cell withShadow cell--row-2 cell--col-1")#type:ignore
    news_without_img = container.find_all("div", class_="style_card__iYFwf knb-card knb-grid-cell withShadow cell--row-2 cell--col-1")#type:ignore
    all_links = []

    for a in big_news:
        link = "https://www.igromania.ru" + a.find("a",class_="knb-card--image style_wrap___iepK style_isAbsolute__P_sj_").get("href")#type:ignore
        all_links.append(link) #type:ignore
    
    for a in little_news:
        link = "https://www.igromania.ru" + a.find("a").get("href")#type:ignore
        all_links.append(link) #type:ignore
    
    for a in news_without_img:
        link = "https://www.igromania.ru" + a.find("a").get("href")#type:ignore
        all_links.append(link) #type:ignore

    all_links = list(set(all_links))
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(all_links)}")
    if len(all_links) == 0:
        get_links()
    else:
        return all_links

def extract_id(url: str) -> str | None:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ–≤–æ–π ID –∏–∑ —Å—Å—ã–ª–æ–∫ –ò–≥—Ä–æ–º–∞–Ω–∏–∏:
    /news/<id>/..., /review/<id>/..., /article/<id>/...
    """
    match = re.search(r"/(?:news|review|article)/(\d+)/", url)
    return match.group(1) if match else None

def get_title(news_url):
    """–ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏."""
    response = safe_get(news_url)
    if not response:
        return None

    soup = BeautifulSoup(response.text, "lxml")
    news = {
        "id": "",
        "title": "",
        "content": ""
    }
    
    news["id"] = str(extract_id(news_url))
    h1 = soup.find("h1")
    if h1:
        news["title"] = h1.text
    content_grid = soup.find("div",class_="d-grid template-columns-5 gap-20 w-100")
    content_text = content_grid.find_all("p") #type:ignore
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
    raw_text = "\n\n".join(p.get_text(" ", strip=True) for p in content_text)

    clean_text = re.sub(r"–ò—Å—Ç–æ—á–Ω–∏–∫:.*?(?=\n|$)", "", raw_text)
    clean_text = re.sub(r"\s+", " ", clean_text).strip()

    news["content"] = clean_text
    return news 


# –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
if __name__ == "__main__":
    print("üì° –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
    news_links = get_links()

    if not news_links:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫–∏. –í–æ–∑–º–æ–∂–Ω–æ, —Å–∞–π—Ç –∏–∑–º–µ–Ω–∏–ª —Å—Ç—Ä—É–∫—Ç—É—Ä—É.")
        exit()

    news_list = []
    
    for link in news_links:  # –æ–≥—Ä–∞–Ω–∏—á–∏–º 10 –Ω–æ–≤–æ—Å—Ç—è–º–∏
        print(f"\nüì∞ –ü–∞—Ä—Å–∏–º: {link}")
        time.sleep(random.uniform(4, 8))  # –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        content = get_title(link)
        if content:
            news_list.append(content)
            print(f"‚Üí {content}")
        else:
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫")

    print("\n=== –ò—Ç–æ–≥–æ–≤—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ ===")
    for t in news_list:
        print("‚Ä¢", t)
